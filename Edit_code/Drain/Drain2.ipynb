{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khai báo sử dụng các thư viện. Thực hiện chạy ngay trên CPU, do đó, chỉ thực hiện mô phỏng, nếu ổn định sẽ thực hiện chạy trên kaggle\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import sys\n",
    "\n",
    "import regex as re\n",
    "import os\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "import copy\n",
    "\n",
    "from tabulate import tabulate # Thư viện sử dụng hiển thị dữ liệu dạng bảng cho DataFrame\n",
    "\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chỉnh sửa class Logcluster:\n",
    "from Root_Drain import LogParser, Node, Logcluster            # Source code cũ của Drain\n",
    "\n",
    "class Logcluster3(Logcluster):\n",
    "    \"\"\"\n",
    "    Class được chỉnh sửa lại, bao gồm một số thông số như sau:\n",
    "    \n",
    "    Thuộc tính:\n",
    "    ----------\n",
    "        - `logTemplate`: Template đặc trưng đại diện cho nhóm log đó, [\"The1\", \"the2\", ...]\n",
    "        - `logIDL`     : Danh sách các ID message log thuộc nhóm log trên, [1,2,3,4,5, ...] \n",
    "        - `levelL`     : Danh sách các Level (INFO, WARN, FATAL, ERROR) mà nhóm log có thể biểu diễn, [\"WARN\", \"INFO\", ...]\n",
    "        - `idTemplate` : Mã định danh của template đó, có thể bị thay đổi do template.\n",
    "        - `idLevelTemp`: Xác định mã định danh theo dạng {\"INFO\": \"L1\", ...} \n",
    "    \"\"\"\n",
    "    def __init__(self, logTemplate=\"\", levelL=None, logIDL=None):\n",
    "        self.logTemplate = logTemplate\n",
    "        if logIDL is None:\n",
    "            logIDL = []\n",
    "        self.logIDL = logIDL\n",
    "        self.levelL = list(levelL)\n",
    "        self.idTemplate = None\n",
    "        self.idLevelTemp = {\"TEMP\":None}\n",
    "        \n",
    "    def addIDLevel(self, logLevel):\n",
    "        self.levelL.append(logLevel)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thực hiện chỉnh sửa lại 2 phương thức outputResult() và parser() của Drain:\n",
    "# Kế thừa Drain bằng cách tạo ra class Drain2:\n",
    "class LogParser2(LogParser):\n",
    "    \"\"\"\n",
    "    Kế thừa từ LogParser được sử dụng để thực hiện sửa đổi lại sao cho phù hợp.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)       # Đọc thêm: https://viblo.asia/p/python-args-va-kwargs-gDVK2pdnlLj\n",
    "        self.rootNode = Node()\n",
    "        self.list_logClust = []\n",
    "        self.list_eventsID = {}\n",
    "        self.df_TemplateL = None\n",
    "        self.list_counts = {\"TEMP\":0}\n",
    "    \n",
    "    def convert_unixtime(self, row):\n",
    "        \"\"\"\n",
    "        Phương thức trích xuất thời gian theo dạng chuẩn UNIX\n",
    "        \"\"\"\n",
    "        date_str = row[\"Date\"]\n",
    "        time_str = row[\"Time\"]\n",
    "        datetime_str = date_str[:2] + \"-\" + date_str[2:4] + \"-\" + date_str[4:] + \" \" + time_str[:2] + \":\" + time_str[2:4] + \":\" + time_str[4:]\n",
    "        unixtime = datetime.strptime(datetime_str, \"%m-%y-%d %H:%M:%S\").timestamp()\n",
    "        return int(unixtime)\n",
    "    \n",
    "    # def extract_block_id(self, parameter_list):\n",
    "    #     \"\"\"\n",
    "    #     Phương thức trích xuất blockID cho từng thông điệp log\n",
    "    #     \"\"\"\n",
    "    #     block_ids = []\n",
    "    #     pattern = re.compile(r'\\bblk_-?\\d+\\b')  # Biểu thức chính quy để tìm blockID\n",
    "    #     for param in parameter_list:\n",
    "    #         matches = pattern.findall(param)\n",
    "    #         if matches:  # Kiểm tra nếu tìm thấy blockID\n",
    "    #             block_ids.extend(matches)\n",
    "    #     return block_ids\n",
    "\n",
    "    def extract_block_id(self, parameter_list):\n",
    "        \"\"\"\n",
    "        Phương thức trích xuất blockID cho từng thông điệp log\n",
    "\n",
    "        Trả về:\n",
    "            - Chuỗi blockID đầu tiên được tìm thấy hoặc `NaN` nếu không tìm thấy.\n",
    "        \"\"\"\n",
    "        pattern = re.compile(r'\\bblk_-?\\d+\\b')  # Biểu thức chính quy để tìm blockID\n",
    "        for param in parameter_list:\n",
    "            matches = pattern.findall(param)\n",
    "            if matches:  # Kiểm tra nếu tìm thấy blockID\n",
    "                return matches[0]\n",
    "        return np.nan\n",
    "\n",
    "    def extractByBlockID(self):\n",
    "        \"\"\"\n",
    "            Trích xuất ra quy trình template sử dụng theo BlockID\n",
    "        \"\"\"\n",
    "        unique_values = self.df_log['BlockID'].unique()\n",
    "        unique_values = [x for x in unique_values if pd.notna(x)]\n",
    "\n",
    "        res = {}\n",
    "\n",
    "        for value in unique_values:\n",
    "            filtered_df = self.df_log[self.df_log['BlockID'] == value].sort_values(by='UnixTime')\n",
    "            res[value] = filtered_df['IDLevel'].values.tolist()\n",
    "        \n",
    "        return res\n",
    "    \n",
    "    def outputResult(self, logClustL):\n",
    "        \"\"\"\n",
    "        Thực hiện in thông tin kết quả ra ngoài sau khi hoàn thành quá trình phân tích\n",
    "        \n",
    "        Tham số:\n",
    "        --------\n",
    "            - `logClustL`   : Danh sách các nhóm log đã thu thập được trong quá trình phân tách\n",
    "        \"\"\"\n",
    "        # ----------- Khai báo các biến sử dụng ----------#\n",
    "        log_templates = [0] * self.df_log.shape[0]\n",
    "        log_templateids = [0] * self.df_log.shape[0]\n",
    "        list_events = []                            # Mảng list_events lưu giữ các giá trị template\n",
    "        dictionary_events = self.list_eventsID      # Dictionary lưu giữ các đối tượng idTemplate và các thông số của nó: \"idTemplate\": {\"TEMP\":\"L5\", \"INFO\":\"I2\", ...}\n",
    "    \n",
    "        # --------------Thực hiện phân tích--------------#\n",
    "        # * Duyệt qua từng đối tượng Logcluster để thêm các giá trị vào từng biến trên\n",
    "        for logClust in logClustL:\n",
    "            template_str = \" \".join(logClust.logTemplate)\n",
    "            occurrence = len(logClust.logIDL)\n",
    "            template_id = hashlib.md5(template_str.encode(\"utf-8\")).hexdigest()[0:8]\n",
    "            logClust.idTemplate = template_id   # không cần cũng được\n",
    "            for logID in logClust.logIDL:\n",
    "                logID -= 1\n",
    "                log_templates[logID] = template_str\n",
    "                log_templateids[logID] = template_id\n",
    "            logClust.logIDL=[]\n",
    "            list_events.append([template_id, template_str, occurrence, logClust.levelL, logClust.idLevelTemp])\n",
    "            if template_id not in dictionary_events:\n",
    "                dictionary_events[template_id] = logClust.idLevelTemp\n",
    "\n",
    "        # DataFrame df_event lưu giữ các thông tin của Template\n",
    "        df_event = pd.DataFrame(\n",
    "            list_events, columns=[\"EventId\", \"EventTemplate\", \"Occurrences\", \"logLevel\", \"IDlogLevel\"]\n",
    "        )\n",
    "        # varL1.df_all_events = df_event\n",
    "        \n",
    "        # DataFrame df_log lưu giữ các thông tin của từng dòng thông điệp log\n",
    "        self.df_log[\"EventId\"] = log_templateids\n",
    "        self.df_log[\"EventTemplate\"] = log_templates\n",
    "        \n",
    "        # Chuyển đổi thời gian theo dạng UnixTime:\n",
    "        # Tạo thêm cột \"UnixTime\" trong DataFrame: df_log\n",
    "        self.df_log[\"UnixTime\"] = self.df_log.apply(self.convert_unixtime, axis=1)\n",
    "                \n",
    "        if self.keep_para:\n",
    "            self.df_log[\"ParameterList\"] = self.df_log.apply(\n",
    "                self.get_parameter_list, \n",
    "                axis=1\n",
    "            )\n",
    "        self.df_log[\"IDLevel\"] = self.df_log.apply(\n",
    "            lambda row: dictionary_events.get(row[\"EventId\"], {}).get(row[\"Level\"], None),\n",
    "            axis=1\n",
    "        )\n",
    "        self.df_log[\"LineId2\"] = self.df_log.apply(\n",
    "            lambda row: dictionary_events.get(row[\"EventId\"], {}).get(\"TEMP\", None),\n",
    "            axis=1\n",
    "        )\n",
    "        self.df_log[\"BlockID\"] = self.df_log[\"ParameterList\"].apply(self.extract_block_id)\n",
    "        \n",
    "        result_list = self.extractByBlockID()\n",
    "        result_df = pd.DataFrame(list(result_list.items()), columns=['BlockID', 'Process'])\n",
    "        result_df['Process'] = result_df['Process'].apply(lambda x: ', '.join(map(str, x)))\n",
    "        \n",
    "        # self.df_log.to_csv(\n",
    "        #     os.path.join(self.savePath, self.logName + \"_structured.csv\"), index=False\n",
    "        # )\n",
    "        result_df.to_csv(\n",
    "            os.path.join(self.savePath, self.logName.replace('.log', '') + \"_process.csv\"),\n",
    "            index=False\n",
    "        )\n",
    "    \n",
    "    def parse(self, logName):\n",
    "        \"\"\"\n",
    "        Phương thức thực hiện quá trình phân tích log\n",
    "        \n",
    "        Tham số: \n",
    "        --------\n",
    "            - `logName`: Tên file đầu vào thực hiện phân tích\n",
    "        \"\"\"\n",
    "        print(\"File đầu vào: \" + os.path.join(self.path, logName))\n",
    "        start_time = datetime.now()\n",
    "        self.logName = logName\n",
    "\n",
    "        # ------Gán lại các đối tượng sử dụng--------#\n",
    "        rootNode = self.rootNode       # Địa chỉ Node gốc\n",
    "        logCluL = self.list_logClust   # Lưu giữ các đối tượng nhóm log Logcluster2 trước đó.\n",
    "        templateLogCluL = []           # Lưu giữ các template đã có trước đó.\n",
    "\n",
    "        # ----------------Load dữ liệu---------------#\n",
    "        self.load_data() \n",
    "\n",
    "        #-----------------Phân tích------------------#\n",
    "        # * Gán giá trị cho danh sách đếm template:\n",
    "        unique_Level = self.df_log[\"Level\"].unique()\n",
    "        for value in unique_Level:\n",
    "            if value not in self.list_counts:\n",
    "                self.list_counts[value] = 0\n",
    "\n",
    "        # * Thực hiện phân tích và lấy Template cho từng dòng thông điệp log\n",
    "        count = 0\n",
    "        for idx, line in self.df_log.iterrows(): \n",
    "            logLevel = str(line[\"Level\"])\n",
    "            logID = line[\"LineId\"]\n",
    "            logmessageL = self.preprocess(line[\"Content\"]).strip().split()  \n",
    "            matchCluster = self.treeSearch(rootNode, logmessageL)           \n",
    "            if matchCluster is None:\n",
    "                newCluster = Logcluster3(logTemplate=logmessageL, levelL=[logLevel], logIDL=[logID])\n",
    "                \n",
    "                # - Nếu có template mới, tạo ra, tăng biến đếm, gán id level template cho mẫu:\n",
    "                self.list_counts[\"TEMP\"] += 1\n",
    "                self.list_counts[logLevel] += 1\n",
    "                newCluster.idLevelTemp[\"TEMP\"] = \"L\"+ str(self.list_counts[\"TEMP\"])\n",
    "                newCluster.idLevelTemp[logLevel] = logLevel[0] + str(self.list_counts[logLevel])\n",
    "                \n",
    "                # - Thêm chúng vào cây phân tích\n",
    "                logCluL.append(newCluster)\n",
    "                self.addSeqToPrefixTree(rootNode, newCluster)\n",
    "            else:\n",
    "                # - Nếu template đã có:\n",
    "                newTemplate = self.getTemplate(logmessageL, matchCluster.logTemplate)\n",
    "                matchCluster.logIDL.append(logID)\n",
    "                # + Nếu có sự thay đổi trong template thành một template mới, cập nhật\n",
    "                if \" \".join(newTemplate) != \" \".join(matchCluster.logTemplate):\n",
    "                    matchCluster.logTemplate = newTemplate\n",
    "                    \n",
    "                # + Nếu có thêm một level mới biểu diễn template này, cập nhật nó\n",
    "                if logLevel not in matchCluster.levelL:\n",
    "                    matchCluster.addIDLevel(logLevel)\n",
    "                    self.list_counts[logLevel] += 1\n",
    "                    matchCluster.idLevelTemp[logLevel] = logLevel[0] + str(self.list_counts[logLevel])\n",
    "                    \n",
    "            count += 1\n",
    "            if count % 100000 == 0 or count == len(self.df_log):\n",
    "                print(\"Processed {0:.1f}% of log lines.\".format(count * 100.0 / len(self.df_log))) # Chỉ sửa mỗi vị trí này.\n",
    "\n",
    "        if not os.path.exists(self.savePath):\n",
    "            os.makedirs(self.savePath)\n",
    "        self.outputResult(self.list_logClust)\n",
    "        print(\"Phân tích hoàn thành! [Thời gian thực hiện: {!s}]\".format(datetime.now() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File đầu vào: ../../Dataset/HDFS/v1/test1.log\n",
      "Total lines:  200500\n",
      "Processed 49.9% of log lines.\n",
      "Processed 99.8% of log lines.\n",
      "Processed 100.0% of log lines.\n",
      "Phân tích hoàn thành! [Thời gian thực hiện: 0:05:46.708125]\n",
      "25\n",
      "200500\n",
      "{'TEMP': 25, 'INFO': 24, 'WARN': 1}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Usecols do not match columns, columns expected but not found: ['Anomaly', 'BlockID']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 36\u001b[0m\n\u001b[0;32m     33\u001b[0m usecols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBlockID\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnomaly\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Read data with subset of columns\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m anomaly_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./HDFS.anomaly_label.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musecols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(anomaly_data))\n",
      "File \u001b[1;32me:\\SOFTWARE\\PYTHON\\PYTHON310\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\SOFTWARE\\PYTHON\\PYTHON310\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32me:\\SOFTWARE\\PYTHON\\PYTHON310\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\SOFTWARE\\PYTHON\\PYTHON310\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1723\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1720\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1722\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1723\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1724\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1725\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32me:\\SOFTWARE\\PYTHON\\PYTHON310\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:140\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39musecols_dtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstring\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mset\u001b[39m(usecols)\u001b[38;5;241m.\u001b[39missubset(\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_names\n\u001b[0;32m    139\u001b[0m ):\n\u001b[1;32m--> 140\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_usecols_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morig_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnames) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlen\u001b[39m(usecols):  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[1;32me:\\SOFTWARE\\PYTHON\\PYTHON310\\Lib\\site-packages\\pandas\\io\\parsers\\base_parser.py:969\u001b[0m, in \u001b[0;36mParserBase._validate_usecols_names\u001b[1;34m(self, usecols, names)\u001b[0m\n\u001b[0;32m    967\u001b[0m missing \u001b[38;5;241m=\u001b[39m [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m usecols \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m names]\n\u001b[0;32m    968\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 969\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    970\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsecols do not match columns, columns expected but not found: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    971\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    972\u001b[0m     )\n\u001b[0;32m    974\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m usecols\n",
      "\u001b[1;31mValueError\u001b[0m: Usecols do not match columns, columns expected but not found: ['Anomaly', 'BlockID']"
     ]
    }
   ],
   "source": [
    "input_dir  = '../../Dataset/HDFS/v1/'   # Đường dẫn vào thư mục chứa file đầu vào\n",
    "output_dir = 'Result/'                  # Thư mục kết quả\n",
    "log_file   = 'test2.log'                 # Tên file đầu vào\n",
    "log_format = '<Date> <Time> <Pid> <Level> <Component>: <Content>'  # HDFS log format\n",
    "# Biểu thức chính quy\n",
    "regex      = [\n",
    "    r'blk_(|-)[0-9]+' , # block id\n",
    "    r'(/|)([0-9]+\\.){3}[0-9]+(:[0-9]+|)(:|)', # IP\n",
    "    r'(?<=[^A-Za-z0-9])(\\-?\\+?\\d+)(?=[^A-Za-z0-9])|[0-9]+$', # Numbers\n",
    "]\n",
    "st         = 0.5  # Ngưỡng tương đồng\n",
    "depth      = 4  # Độ sâu của cây phân tích\n",
    "\n",
    "parserObj = LogParser2(log_format, indir=input_dir, outdir=output_dir, depth=depth, st=st, rex=regex)\n",
    "# parserObj.parse(log_file)\n",
    "\n",
    "selected_columns = ['LineId', 'UnixTime', 'Level', 'EventId', 'EventTemplate', 'IDLevel', 'LineId2', 'ParameterList', 'BlockID']\n",
    "# for i in range(1,6):\n",
    "#     str_file = \"test\"+str(i)+\".log\"\n",
    "#     parserObj.parse(str_file)\n",
    "#     print(len(parserObj.list_logClust))\n",
    "#     print(parserObj.list_counts)\n",
    "#     # outputResult2(parserObj.list_logClust)\n",
    "#     # parserObj.df_log.loc[:, selected_columns].head(60)\n",
    "#     parserObj.df_log = pd.DataFrame()\n",
    "\n",
    "parserObj.parse(\"test1.log\")\n",
    "print(len(parserObj.list_logClust))\n",
    "print(len(parserObj.df_log))\n",
    "print(parserObj.list_counts)\n",
    "# parserObj.df_log.loc[:, selected_columns].head(60)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BlockID</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blk_-1608999687919862906</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blk_7503483334202473044</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blk_-3544583377289625738</td>\n",
       "      <td>Anomaly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blk_-9073992586687739851</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blk_7854771516489510256</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>blk_1717858812220360316</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>blk_-2519617320378473615</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>blk_7063315473424667801</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>blk_8586544123689943463</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>blk_2765344736980045501</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>blk_-2900490557492272760</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>blk_-50273257731426871</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>blk_4394112519745907149</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>blk_3640100967125688321</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>blk_-40115644493265216</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>blk_-8531310335568756456</td>\n",
       "      <td>Anomaly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>blk_-3409923645141256069</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>blk_3974948352784823938</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>blk_5647760196018207394</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>blk_-202775138379690649</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     BlockID    Label\n",
       "0   blk_-1608999687919862906   Normal\n",
       "1    blk_7503483334202473044   Normal\n",
       "2   blk_-3544583377289625738  Anomaly\n",
       "3   blk_-9073992586687739851   Normal\n",
       "4    blk_7854771516489510256   Normal\n",
       "5    blk_1717858812220360316   Normal\n",
       "6   blk_-2519617320378473615   Normal\n",
       "7    blk_7063315473424667801   Normal\n",
       "8    blk_8586544123689943463   Normal\n",
       "9    blk_2765344736980045501   Normal\n",
       "10  blk_-2900490557492272760   Normal\n",
       "11    blk_-50273257731426871   Normal\n",
       "12   blk_4394112519745907149   Normal\n",
       "13   blk_3640100967125688321   Normal\n",
       "14    blk_-40115644493265216   Normal\n",
       "15  blk_-8531310335568756456  Anomaly\n",
       "16  blk_-3409923645141256069   Normal\n",
       "17   blk_3974948352784823938   Normal\n",
       "18   blk_5647760196018207394   Normal\n",
       "19   blk_-202775138379690649   Normal"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "575061\n"
     ]
    }
   ],
   "source": [
    "# Read data with subset of columns\n",
    "anomaly_data = pd.read_csv(\"./HDFS.anomaly_label.csv\")\n",
    "anomaly_data.rename(columns={anomaly_data.columns[0]: \"BlockID\"}, inplace=True)\n",
    "anomaly_data.head(20)\n",
    "print(len(anomaly_data))\n",
    "# merged_df_left = pd.merge(anomaly_data, result, on='blockId', how='outer')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
