{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing file: ./filelog1copy.log\n",
      "Total lines:  30000\n",
      "Processed 33.3% of log lines.\n",
      "Processed 66.7% of log lines.\n",
      "Processed 100.0% of log lines.\n",
      "logTemplate: STARTUP_MSG:\n",
      "logTemplate: registeredUNIXsignalhandlersfor[TERM,HUP,INT]\n",
      "logTemplate: loadedpropertiesfromhadoop-metrics2.properties\n",
      "logTemplate: Scheduledsnapshotperiodat<*>second(s).\n",
      "logTemplate: DataNodemetricssystemstarted\n",
      "logTemplate: InitializedblockscannerwithtargetBytesPerSec<*>\n",
      "logTemplate: Configuredhostnameismesos-master<*>\n",
      "logTemplate: StartingDataNodewithmaxLockedMemory=<*>\n",
      "logTemplate: Opened<*>serverat<*>\n",
      "logTemplate: Balancingbandwithis<*>bytes/s\n",
      "logTemplate: Numberthreadsforbalancingis<*>\n",
      "logTemplate: Loggingtoorg.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log)viaorg.mortbay.log.Slf4jLog\n",
      "logTemplate: UnabletoinitializeFileSignerSecretProvider,fallingbacktouserandomsecrets.\n",
      "logTemplate: Httprequestlogforhttp.requests.datanodeisnotdefined\n",
      "logTemplate: Addedglobalfilter'safety'(class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\n",
      "logTemplate: Addedfilterstatic_user_filter(class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter)tocontext<*>\n",
      "logTemplate: Jettyboundtoport<*>\n",
      "logTemplate: jetty<*>\n",
      "logTemplate: StartedHttpServer2$SelectChannelConnectorWithSafeStartup@localhost:<*>\n",
      "logTemplate: ListeningHTTPtrafficon<*>\n",
      "logTemplate: dnUserName=hdfs\n",
      "logTemplate: supergroup=hdfsgroup\n",
      "logTemplate: UsingcallQueueclassjava.util.concurrent.LinkedBlockingQueue\n",
      "logTemplate: StartingSocketReader#<*>forport<*>\n",
      "logTemplate: Refreshrequestreceivedfornameservices:null\n",
      "logTemplate: StartingBPOfferServicesfornameservices:<default>\n",
      "logTemplate: Blockpool<*>(DatanodeUuid<*>servicetomesos-master<*><*><*><*><*><*>\n",
      "logTemplate: IPCServerResponder:starting\n",
      "logTemplate: IPCServerlisteneron<*>:starting\n",
      "logTemplate: Lockon<*>acquiredbynodename<*>@mesos-master<*>\n",
      "logTemplate: Storagedirectory<*>isnotformattedforBP<*>\n",
      "logTemplate: Formatting...\n",
      "logTemplate: AnalyzingstoragedirectoriesforbpidBP<*>\n",
      "logTemplate: Lockingisdisabledfor<*>\n",
      "logTemplate: Blockpoolstoragedirectory<*>isnotformattedforBP<*>\n",
      "logTemplate: FormattingblockpoolBP<*>directory<*>\n",
      "logTemplate: Restored<*>blockfilesfromtrash.\n",
      "logTemplate: Settingupstorage:nsid=<*>;bpid=BP<*>;lv=<*>;nsInfo=lv=<*>;cid=CID-<*>;nsid=<*>;c=<*>;bpid=BP<*>;dnuuid=null\n",
      "logTemplate: GeneratedandpersistednewDatanodeUUID<*>\n",
      "logTemplate: Addednewvolume:DS-<*>\n",
      "logTemplate: Addedvolume-<*>,StorageType:DISK\n",
      "logTemplate: RegisteredFSDatasetStateMBean\n",
      "logTemplate: AddingblockpoolBP<*>\n",
      "logTemplate: ScanningblockpoolBP<*>onvolume<*>\n",
      "logTemplate: TimetakentoscanblockpoolBP<*>on<*>:<*>ms\n",
      "logTemplate: TotaltimetoscanallreplicasforblockpoolBP<*>:<*>ms\n",
      "logTemplate: AddingreplicastomapforblockpoolBP<*>onvolume<*>\n",
      "logTemplate: TimetoaddreplicastomapforblockpoolBP<*>onvolume<*>:<*>ms\n",
      "logTemplate: Totaltimetoaddallreplicastomap:<*>ms\n",
      "logTemplate: NowscanningbpidBP<*>onvolume<*>\n",
      "logTemplate: PeriodicDirectoryTreeVerificationscanstartingat<*>withinterval<*>\n",
      "logTemplate: VolumeScanner(<*>,DS-<*>):finishedscanningblockpoolBP<*>\n",
      "logTemplate: BlockpoolBlockpoolBP<*>(DatanodeUuidnull)servicetomesos-master<*><*>successfullyregisteredwithNN\n",
      "logTemplate: Fornamenodemesos-master<*><*>usingDELETEREPORT_INTERVALof<*>msecBLOCKREPORT_INTERVALof<*>msecCACHEREPORT_INTERVALof<*>msecInitialdelay:<*>msec;heartBeatInterval=<*>\n",
      "logTemplate: NamenodeBlockpoolBP<*>(DatanodeUuid<*>)servicetomesos-master<*><*>tryingtoclaimACTIVEstatewithtxid=<*>\n",
      "logTemplate: AcknowledgingACTIVENamenodeBlockpoolBP<*>(DatanodeUuid<*>)servicetomesos-master<*><*>\n",
      "logTemplate: VolumeScanner(<*>,DS-<*>):nosuitableblockpoolsfoundtoscan.Waiting<*>ms.\n",
      "logTemplate: Successfullysentblockreport<*>,containing<*>storagereport(s),ofwhichwesent<*>.Thereportshad<*>totalblocksandused<*>RPC(s).Thistook<*>msectogenerateand<*>msecsforRPCandNNprocessing.Gotbackonecommand:FinalizeCommand<*>\n",
      "logTemplate: GotfinalizecommandforblockpoolBP<*>\n",
      "logTemplate: BlockPoolBP<*>Totalblocks:<*>,missingmetadatafiles:<*>,missingblockfiles:<*>,missingblocksinmemory:<*>,mismatchedblocks:<*>\n",
      "logTemplate: NowrescanningbpidBP<*>onvolume<*>,aftermorethan<*>hour(s)\n",
      "logTemplate: ReceivingBP<*>:<*>src:<*>dest:<*>\n",
      "logTemplate: src:<*>,dest:<*>,bytes:<*>,op:HDFS_WRITE,cliID:<*>offset:<*>,srvID:<*>,blockid:BP<*>:<*>,duration:<*>\n",
      "logTemplate: PacketResponder:BP<*>:<*>,type=HAS_DOWNSTREAM_IN_PIPELINEterminating\n",
      "logTemplate: Scheduling<*>file<*>fordeletion\n",
      "logTemplate: DeletedBP<*><*>file<*>\n",
      "logTemplate: PacketResponder:BP<*>:<*>,type=LAST_IN_PIPELINE,downstreams=<*>:[]terminating\n",
      "logTemplate: ReceivedBP<*>:<*>src:<*>dest:<*>ofsize<*>\n",
      "logTemplate: VolumeScanner(<*>,DS-<*>):SchedulingsuspectblockBP<*>:<*>forrescanning.\n",
      "logTemplate: VolumeScanner(<*>,DS-<*>):NotschedulingsuspectblockBP<*>:<*>forrescanning,becausewerescanneditrecently.\n",
      "logTemplate: VolumeScanner(<*>,DS-<*>):suspectblockBP<*>:<*>isalreadyqueuedforrescanning.\n",
      "logTemplate: SlowBlockReceiverwritepackettomirrortook<*>ms(threshold=<*>ms)\n",
      "\t\tRoot\n",
      "\t\t\t<1>\n",
      "\t\t\t\tSTARTUP_MSG:\n",
      "\t\t\t\tjetty<*>\n",
      "\t\t\t<8>\n",
      "\t\t\t\tregistered\n",
      "\t\t\t\tHttp\n",
      "\t\t\t\tStorage\n",
      "\t\t\t\tVolumeScanner(<*>,\n",
      "\t\t\t<4>\n",
      "\t\t\t\tloaded\n",
      "\t\t\t\tDataNode\n",
      "\t\t\t\tConfigured\n",
      "\t\t\t\tUsing\n",
      "\t\t\t\tIPC\n",
      "\t\t\t\tSetting\n",
      "\t\t\t\tAdded\n",
      "\t\t\t\tAdding\n",
      "\t\t\t\tPacketResponder:\n",
      "\t\t\t<6>\n",
      "\t\t\t\tScheduled\n",
      "\t\t\t\tInitialized\n",
      "\t\t\t\tStarting\n",
      "\t\t\t\tNumber\n",
      "\t\t\t\tRefresh\n",
      "\t\t\t\tIPC\n",
      "\t\t\t\tAnalyzing\n",
      "\t\t\t\tFormatting\n",
      "\t\t\t\tRestored\n",
      "\t\t\t\tAdded\n",
      "\t\t\t\tReceiving\n",
      "\t\t\t\tScheduling\n",
      "\t\t\t<5>\n",
      "\t\t\t\tOpened\n",
      "\t\t\t\tBalancing\n",
      "\t\t\t\tLogging\n",
      "\t\t\t\tAdded\n",
      "\t\t\t\tJetty\n",
      "\t\t\t\tListening\n",
      "\t\t\t\tStarting\n",
      "\t\t\t\tLocking\n",
      "\t\t\t\tDeleted\n",
      "\t\t\t\tPacketResponder:\n",
      "\t\t\t<10>\n",
      "\t\t\t\tUnable\n",
      "\t\t\t\tBlock\n",
      "\t\t\t\tTime\n",
      "\t\t\t\tVolumeScanner(<*>,\n",
      "\t\t\t<7>\n",
      "\t\t\t\tAdded\n",
      "\t\t\t\tStarting\n",
      "\t\t\t\tLock\n",
      "\t\t\t\tGenerated\n",
      "\t\t\t\tScanning\n",
      "\t\t\t\tNow\n",
      "\t\t\t\tVolumeScanner(<*>,\n",
      "\t\t\t\tGot\n",
      "\t\t\t<2>\n",
      "\t\t\t\tStarted\n",
      "\t\t\t\tFormatting\n",
      "\t\t\t<3>\n",
      "\t\t\t\tdnUserName\n",
      "\t\t\t\tsupergroup\n",
      "\t\t\t\tRegistered\n",
      "\t\t\t<13>\n",
      "\t\t\t\tBlock\n",
      "\t\t\t<11>\n",
      "\t\t\t\tTotal\n",
      "\t\t\t\tAdding\n",
      "\t\t\t\tPeriodic\n",
      "\t\t\t<14>\n",
      "\t\t\t\tTime\n",
      "\t\t\t\tVolumeScanner(<*>,\n",
      "\t\t\t<9>\n",
      "\t\t\t\tTotal\n",
      "\t\t\t\tReceived\n",
      "\t\t\t\tSlow\n",
      "\t\t\t<15>\n",
      "\t\t\t\tBlock\n",
      "\t\t\t<18>\n",
      "\t\t\t\tFor\n",
      "\t\t\t\tsrc:\n",
      "\t\t\t<17>\n",
      "\t\t\t\tNamenode\n",
      "\t\t\t\tBlockPool\n",
      "\t\t\t<12>\n",
      "\t\t\t\tAcknowledging\n",
      "\t\t\t\tVolumeScanner(<*>,\n",
      "\t\t\t\tNow\n",
      "\t\t\t<43>\n",
      "\t\t\t\tSuccessfully\n",
      "Parsing done. [Time taken: 0:00:03.243554]\n"
     ]
    }
   ],
   "source": [
    "# =========================================================================\n",
    "# Copyright (C) 2016-2023 LOGPAI (https://github.com/logpai).\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =========================================================================\n",
    "\n",
    "\n",
    "import regex as re\n",
    "import os\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "\n",
    "class Logcluster:\n",
    "    \"\"\"\n",
    "    Class Logcluster là một đối tượng LOG GROUP, được sử dụng để thực hiện quá trình lưu trữ thông tin của một nhóm log, bao gồm: \n",
    "    \n",
    "    Thuộc tính:\n",
    "    -----------\n",
    "        - `logTemplate`: Template đặc trưng của nhóm log đó\n",
    "        - `logIDL`: Danh sách các ID message log thuộc nhóm log trên\n",
    "    \"\"\"\n",
    "    def __init__(self, logTemplate=\"\", logIDL=None):\n",
    "        self.logTemplate = logTemplate\n",
    "        if logIDL is None:\n",
    "            logIDL = []\n",
    "        self.logIDL = logIDL\n",
    "\n",
    "class Node:\n",
    "    \"\"\"\n",
    "    Class Node là các node có trong cây phân tích - parser tree. Parser Tree có 01 Root node, internal node lớp 1 là các node chứa chiều dài của chuỗi, tiếp theo là các internal node token, cuối cùng là leaf node.\n",
    "    \n",
    "    Thuộc tính:\n",
    "    ----------\n",
    "    - `childD`: Các Node của Node hiện tại - là một dictionary đối với internal node và list với leaf node\n",
    "    - `depth`: Độ sâu của node hiện tại\n",
    "    - `digitOrtoken`: Token mà node hiện tại đang chứa\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, childD=None, depth=0, digitOrtoken=None):\n",
    "        if childD is None:\n",
    "            childD = dict()\n",
    "        self.childD = childD\n",
    "        self.depth = depth\n",
    "        self.digitOrtoken = digitOrtoken\n",
    "\n",
    "class LogParser:\n",
    "    \"\"\"\n",
    "        Class `LogParser` bao gồm các phương thức hỗ trợ quá trình phân tích log.\n",
    "        \n",
    "        Thuộc tính:\n",
    "        -------------\n",
    "            - `rex`       : Biểu thức chính quy được sử dụng cho quá tình tiền xử lý (step1). Mặc định: []\n",
    "            - `path`      : Đường dẫn tệp log đầu vào \n",
    "            - `depth`     : Độ sâu của tất cả các Node lá (leaf node). Mặc định: 4 --> 4-2 = 2\n",
    "            - `st`        : Ngưỡng tương đồng (similarity threshold). Mặc định: 0.4\n",
    "            - `maxChild`  : Số lượng tối đa của các node con của một node nội tại (internal node - là một nút trong cây có ít nhất một node con). Mặc định: 100\n",
    "            - `logName`   : Tên tệp đầu vào chứa các thông báo log thô.\n",
    "            - `savePath`  : Đường dẫn đầu ra của tệp chứa các log đã được cấu trúc.\n",
    "            - `keep_para` : Yêu cầu có lưu giữ tham số hay không. Mặc định: true\n",
    "            - `df_log`    :\n",
    "            - `log_format`:  \n",
    "            \n",
    "        Phương thức:\n",
    "        ------------\n",
    "            - \n",
    "        \"\"\"\n",
    "    def __init__(self,\n",
    "        log_format,\n",
    "        indir=\"./\",\n",
    "        outdir=\"./result/\",\n",
    "        depth=4,\n",
    "        st=0.4,\n",
    "        maxChild=100,\n",
    "        rex=[],\n",
    "        keep_para=True,\n",
    "    ):\n",
    "        self.path = indir\n",
    "        self.depth = depth - 2\n",
    "        self.st = st\n",
    "        self.maxChild = maxChild\n",
    "        self.logName = None\n",
    "        self.savePath = outdir\n",
    "        self.df_log = None\n",
    "        self.log_format = log_format\n",
    "        self.rex = rex\n",
    "        self.keep_para = keep_para\n",
    "\n",
    "    def hasNumbers(self, s):\n",
    "        \"\"\"\n",
    "        Phương thức kiểm tra chuỗi đầu vào có chứa ít nhất một ký tự là chữ số hay không\n",
    "\n",
    "        Tham số:\n",
    "            - `s` (_string_): Chuỗi đầu vào cần kiểm tra\n",
    "\n",
    "        Trả về:\n",
    "            _bool_: Kết quả `true` nếu chuỗi đó có ít nhất 1 ký tự là chữ số (0 - 9), ngược lại: `false`\n",
    "        \"\"\"\n",
    "        return any(char.isdigit() for char in s)\n",
    "\n",
    "    def treeSearch(self, rn, seq):\n",
    "        \"\"\"\n",
    "        Phương thức thực hiện tìm kiếm nhóm log phù hợp với Template \"seq\" đang xét, bắt đầu từ RootNode. Kết quả trả về là nhóm log phù hợp với template \"seq\" cho trước hoặc None nếu không tìm thấy\n",
    "\n",
    "        Tham số:\n",
    "            - `rn`: Root Node\n",
    "            - `seq`: Template để thực hiện tìm kiếm nhóm log so khớp\n",
    "\n",
    "        Returns:\n",
    "            Nhóm log phù hợp || None\n",
    "        \"\"\"\n",
    "        retLogClust = None\n",
    "\n",
    "        seqLen = len(seq)\n",
    "        if seqLen not in rn.childD:\n",
    "            return retLogClust\n",
    "\n",
    "        parentn = rn.childD[seqLen]\n",
    "\n",
    "        # Thực hiện tìm kiếm internal node cuối cùng chứa log group phù hợp với \"seq\"\n",
    "        currentDepth = 1\n",
    "        for token in seq:\n",
    "            if currentDepth >= self.depth or currentDepth > seqLen:\n",
    "                break\n",
    "\n",
    "            if token in parentn.childD:\n",
    "                parentn = parentn.childD[token]\n",
    "            elif \"<*>\" in parentn.childD:\n",
    "                parentn = parentn.childD[\"<*>\"]\n",
    "            else:\n",
    "                return retLogClust\n",
    "            currentDepth += 1\n",
    "\n",
    "        logClustL = parentn.childD\n",
    "\n",
    "        # Chọn log group phù hợp nhất:\n",
    "        retLogClust = self.fastMatch(logClustL, seq)\n",
    "\n",
    "        return retLogClust\n",
    "\n",
    "    def addSeqToPrefixTree(self, rn, logClust):\n",
    "        \"\"\"\n",
    "        Phương thức thực hiện thêm nhóm log \"logClust\" cho trước vào một internal Node phù hợp\n",
    "\n",
    "        Tham số:\n",
    "            -`rn`: Bắt đầu từ RootNode\n",
    "            - `logClust`: Nhóm log cần thêm vào\n",
    "        \"\"\"\n",
    "        seqLen = len(logClust.logTemplate)\n",
    "        if seqLen not in rn.childD:\n",
    "            firtLayerNode = Node(depth=1, digitOrtoken=seqLen)\n",
    "            rn.childD[seqLen] = firtLayerNode # length: Node(phù hợp)\n",
    "        else:\n",
    "            firtLayerNode = rn.childD[seqLen]\n",
    "\n",
    "        parentn = firtLayerNode\n",
    "        \n",
    "        # Bắt đầu duyệt các internal node từ lớp thứ 2 để tìm kiếm internal node phù hợp\n",
    "        currentDepth = 1\n",
    "        for token in logClust.logTemplate:\n",
    "            # Add current log cluster to the leaf node\n",
    "            if currentDepth >= self.depth or currentDepth > seqLen:\n",
    "                if len(parentn.childD) == 0:\n",
    "                    parentn.childD = [logClust]\n",
    "                else:\n",
    "                    parentn.childD.append(logClust)\n",
    "                break\n",
    "\n",
    "            # If token not matched in this layer of existing tree.\n",
    "            if token not in parentn.childD:\n",
    "                # Kiểm tra token(từ) đó có chứa chữ số nào không? CÓ: lưu vào <*>: KHÔNG: Lưu như bình thường\n",
    "                if not self.hasNumbers(token):\n",
    "                    if \"<*>\" in parentn.childD:\n",
    "                        if len(parentn.childD) < self.maxChild:\n",
    "                            newNode = Node(depth=currentDepth + 1, digitOrtoken=token)\n",
    "                            parentn.childD[token] = newNode\n",
    "                            parentn = newNode\n",
    "                        else:\n",
    "                            parentn = parentn.childD[\"<*>\"]\n",
    "                    else:\n",
    "                        if len(parentn.childD) + 1 < self.maxChild:\n",
    "                            newNode = Node(depth=currentDepth + 1, digitOrtoken=token)\n",
    "                            parentn.childD[token] = newNode\n",
    "                            parentn = newNode\n",
    "                        elif len(parentn.childD) + 1 == self.maxChild:\n",
    "                            newNode = Node(depth=currentDepth + 1, digitOrtoken=\"<*>\")\n",
    "                            parentn.childD[\"<*>\"] = newNode\n",
    "                            parentn = newNode\n",
    "                        else:\n",
    "                            parentn = parentn.childD[\"<*>\"]\n",
    "\n",
    "                else:\n",
    "                    if \"<*>\" not in parentn.childD:\n",
    "                        newNode = Node(depth=currentDepth + 1, digitOrtoken=\"<*>\")\n",
    "                        parentn.childD[\"<*>\"] = newNode\n",
    "                        parentn = newNode\n",
    "                    else:\n",
    "                        parentn = parentn.childD[\"<*>\"]\n",
    "\n",
    "            # If the token is matched\n",
    "            else:\n",
    "                parentn = parentn.childD[token]\n",
    "\n",
    "            currentDepth += 1\n",
    "\n",
    "    # seq1 is template\n",
    "    def seqDist(self, seq1, seq2):\n",
    "        \"\"\"\n",
    "        Phương thức thực hiện tính toán độ tương đồng của 02 template cho trước\n",
    "\n",
    "        Tham số:\n",
    "            - `seq1`, `seq2` : 02 Template thực hiện so khớp độ tương đồng. Thông thường, `seq1` là template của các nhóm log, `seq2` là template log cần so khớp\n",
    "\n",
    "        Trả về:\n",
    "            - Giá trị (float) tương đồng của 02 template \n",
    "            - Số lượng token `<*>` có trong chuỗi `seq1`\n",
    "        \"\"\"\n",
    "        assert len(seq1) == len(seq2) # Cảnh báo thoát chương trình nếu 2 template không cùng độ dài\n",
    "        simTokens = 0\n",
    "        numOfPar = 0\n",
    "\n",
    "        for token1, token2 in zip(seq1, seq2):\n",
    "            if token1 == \"<*>\":\n",
    "                numOfPar += 1\n",
    "                continue\n",
    "            if token1 == token2:\n",
    "                simTokens += 1\n",
    "\n",
    "        retVal = float(simTokens) / len(seq1)\n",
    "\n",
    "        return retVal, numOfPar\n",
    "\n",
    "    def fastMatch(self, logClustL, seq):\n",
    "        \"\"\"\n",
    "        Phương thức thực hiện tìm kiếm nhóm log phù hợp với \"seq\" cho trước dựa trên mảng các nhóm log \"logClustL\" đã cho\n",
    "\n",
    "        Tham số:\n",
    "         - `logClustL`: Một LIST các nhóm log cho trước để tìm kiếm\n",
    "         - `seq`      : Template log được dùng để so khớp\n",
    "\n",
    "        Trả về:\n",
    "            Nhóm log phù hợp nhất với \"seq\" || None\n",
    "        \"\"\"\n",
    "        retLogClust = None\n",
    "\n",
    "        maxSim = -1\n",
    "        maxNumOfPara = -1\n",
    "        maxClust = None\n",
    "\n",
    "        for logClust in logClustL:\n",
    "            curSim, curNumOfPara = self.seqDist(logClust.logTemplate, seq)\n",
    "            if curSim > maxSim or (curSim == maxSim and curNumOfPara > maxNumOfPara):\n",
    "                maxSim = curSim\n",
    "                maxNumOfPara = curNumOfPara\n",
    "                maxClust = logClust\n",
    "\n",
    "        if maxSim >= self.st:\n",
    "            retLogClust = maxClust\n",
    "\n",
    "        return retLogClust\n",
    "\n",
    "    def getTemplate(self, seq1, seq2):\n",
    "        \"\"\"\n",
    "        Phương thức thực hiện đồng nhất giữa 02 template với nhau, những token khác nhau được thay thế bằng dấu <*>\n",
    "\n",
    "        Tham số:\n",
    "            - `seq1`: Tham số đầu vào, thường là template log phù hợp với nhóm log đã tìm được\n",
    "            - `seq2`: Tham số đầu vào, thường là template của nhóm log tìm được. Phương thức thực hiện để tìm ra template chung nhất để biểu diễn cho 2 template này.\n",
    "\n",
    "        Trả về:\n",
    "            Template chung nhất biểu diễn được cả 02 template trên\n",
    "        \"\"\"\n",
    "        assert len(seq1) == len(seq2) # Kiểm tra chắc chắn seq1 = seq2\n",
    "        retVal = []\n",
    "        i = 0\n",
    "        for word in seq1:\n",
    "            if word == seq2[i]:\n",
    "                retVal.append(word)\n",
    "            else:\n",
    "                retVal.append(\"<*>\")\n",
    "            i += 1\n",
    "        return retVal\n",
    "\n",
    "    def outputResult(self, logClustL):\n",
    "        log_templates = [0] * self.df_log.shape[0]\n",
    "        log_templateids = [0] * self.df_log.shape[0]\n",
    "        df_events = []\n",
    "        for logClust in logClustL:\n",
    "            template_str = \" \".join(logClust.logTemplate)\n",
    "            occurrence = len(logClust.logIDL)\n",
    "            template_id = hashlib.md5(template_str.encode(\"utf-8\")).hexdigest()[0:8]\n",
    "            for logID in logClust.logIDL:\n",
    "                logID -= 1\n",
    "                log_templates[logID] = template_str\n",
    "                log_templateids[logID] = template_id\n",
    "            df_events.append([template_id, template_str, occurrence])\n",
    "\n",
    "        df_event = pd.DataFrame(\n",
    "            df_events, columns=[\"EventId\", \"EventTemplate\", \"Occurrences\"]\n",
    "        )\n",
    "        self.df_log[\"EventId\"] = log_templateids\n",
    "        self.df_log[\"EventTemplate\"] = log_templates\n",
    "        if self.keep_para:\n",
    "            self.df_log[\"ParameterList\"] = self.df_log.apply(\n",
    "                self.get_parameter_list, axis=1\n",
    "            )\n",
    "        self.df_log.to_csv(\n",
    "            os.path.join(self.savePath, self.logName + \"_structured.csv\"), index=False\n",
    "        )\n",
    "\n",
    "        occ_dict = dict(self.df_log[\"EventTemplate\"].value_counts())\n",
    "        df_event = pd.DataFrame()\n",
    "        df_event[\"EventTemplate\"] = self.df_log[\"EventTemplate\"].unique()\n",
    "        df_event[\"EventId\"] = df_event[\"EventTemplate\"].map(\n",
    "            lambda x: hashlib.md5(x.encode(\"utf-8\")).hexdigest()[0:8]\n",
    "        )\n",
    "        df_event[\"Occurrences\"] = df_event[\"EventTemplate\"].map(occ_dict)\n",
    "        df_event.to_csv(\n",
    "            os.path.join(self.savePath, self.logName + \"_templates.csv\"),\n",
    "            index=False,\n",
    "            columns=[\"EventId\", \"EventTemplate\", \"Occurrences\"],\n",
    "        )\n",
    "\n",
    "    def printTree(self, node, dep):\n",
    "        pStr = \"\"\n",
    "        for i in range(dep):\n",
    "            pStr += \"\\t\"\n",
    "\n",
    "        if node.depth == 0:\n",
    "            pStr += \"Root\"\n",
    "        elif node.depth == 1:\n",
    "            pStr += \"<\" + str(node.digitOrtoken) + \">\"\n",
    "        else:\n",
    "            pStr += node.digitOrtoken\n",
    "\n",
    "        print(pStr)\n",
    "\n",
    "        if node.depth == self.depth:\n",
    "            return 1\n",
    "        for child in node.childD:\n",
    "            self.printTree(node.childD[child], dep + 1)\n",
    "\n",
    "    def parse(self, logName):\n",
    "        print(\"Parsing file: \" + os.path.join(self.path, logName))\n",
    "        start_time = datetime.now()\n",
    "        self.logName = logName\n",
    "        rootNode = Node()\n",
    "        logCluL = []\n",
    "\n",
    "        self.load_data()\n",
    "\n",
    "        # Thực hiện quá trình phân tích:\n",
    "        count = 0\n",
    "        for idx, line in self.df_log.iterrows():\n",
    "            logID = line[\"LineId\"]\n",
    "            logmessageL = self.preprocess(line[\"Content\"]).strip().split()\n",
    "            matchCluster = self.treeSearch(rootNode, logmessageL)\n",
    "\n",
    "            # Match no existing log cluster\n",
    "            if matchCluster is None:\n",
    "                newCluster = Logcluster(logTemplate=logmessageL, logIDL=[logID])\n",
    "                logCluL.append(newCluster)\n",
    "                self.addSeqToPrefixTree(rootNode, newCluster)\n",
    "\n",
    "            # Add the new log message to the existing cluster\n",
    "            else:\n",
    "                newTemplate = self.getTemplate(logmessageL, matchCluster.logTemplate)\n",
    "                matchCluster.logIDL.append(logID)\n",
    "                if \" \".join(newTemplate) != \" \".join(matchCluster.logTemplate):\n",
    "                    matchCluster.logTemplate = newTemplate\n",
    "\n",
    "            count += 1\n",
    "            if count % 10000 == 0 or count == len(self.df_log):\n",
    "                print(\n",
    "                    \"Processed {0:.1f}% of log lines.\".format(\n",
    "                        count * 100.0 / len(self.df_log)\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        if not os.path.exists(self.savePath):\n",
    "            os.makedirs(self.savePath)\n",
    "\n",
    "        # self.outputResult(logCluL)\n",
    "        \n",
    "        for obj in logCluL:\n",
    "            for key, value in vars(obj).items():\n",
    "                if key == \"logTemplate\":\n",
    "                    print(f\"{key}: {str(''.join(value))}\")\n",
    "        self.printTree(rootNode, self.depth)        \n",
    "        \n",
    "        print(\"Parsing done. [Time taken: {!s}]\".format(datetime.now() - start_time))\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Phương thức nạp dữ liệu đầu vào để xử lý.\n",
    "        \"\"\"\n",
    "        headers, regex = self.generate_logformat_regex(self.log_format)\n",
    "        self.df_log = self.log_to_dataframe(\n",
    "            os.path.join(self.path, self.logName), regex, headers, self.log_format\n",
    "        )\n",
    "\n",
    "    def preprocess(self, line):\n",
    "        \"\"\"\n",
    "        Phương thức chuyển đổi các message log thành template dựa trên các biểu thức chính quy cung cấp trong `seft.rex`.\n",
    "\n",
    "        Tham số:\n",
    "        ------\n",
    "            line: Message log cần chuyển đổi\n",
    "\n",
    "        Trả về:\n",
    "        -------\n",
    "            Chuỗi đã được chuyển đổi\n",
    "        \"\"\"\n",
    "        for currentRex in self.rex:\n",
    "            line = re.sub(currentRex, \"<*>\", line)\n",
    "        return line\n",
    "\n",
    "    def log_to_dataframe(self, log_file, regex, headers, logformat):\n",
    "        \"\"\"\n",
    "        Phương thức sử dụng để chuyển đổi các message log thành một DataFrame dựa trên các cột cho trước trong headers.\n",
    "\n",
    "        Tham số:\n",
    "        --------\n",
    "            - `log_file`: File đầu vào chứa các message log\n",
    "            - `regex`: Biểu thức chính quy được sử dụng để tìm log phù hợp\n",
    "            - `headers`: Danh sách các tên cột mà ta muốn trích xuất từ dữ liệu log\n",
    "            - `logformat`: Định dạng mong muốn cho log \n",
    "            \n",
    "        Trả về:\n",
    "        --------\n",
    "            `logdf`: là một DataFrame đã được chuyển đổi.\n",
    "        \"\"\"\n",
    "        log_messages = []\n",
    "        linecount = 0\n",
    "        with open(log_file, \"r\") as fin:\n",
    "            for line in fin.readlines():\n",
    "                try:\n",
    "                    match = regex.search(line.strip())\n",
    "                    message = [match.group(header) for header in headers]\n",
    "                    log_messages.append(message)\n",
    "                    linecount += 1\n",
    "                except Exception as e:\n",
    "                    print(\"[Warning] Skip line: \" + line)\n",
    "        logdf = pd.DataFrame(log_messages, columns=headers)\n",
    "        logdf.insert(0, \"LineId\", None)\n",
    "        logdf[\"LineId\"] = [i + 1 for i in range(linecount)]\n",
    "        print(\"Total lines: \", len(logdf))\n",
    "        return logdf\n",
    "\n",
    "    def generate_logformat_regex(self, logformat):\n",
    "        \"\"\"\n",
    "        Phương thức tạo biểu thức chính quy để phân tích log messages và tạo ra danh sách tên của các cột dựa trên đầu vào cho trước.\n",
    "        \n",
    "        Tham số:\n",
    "        ------------\n",
    "        - `logformat`: Các tên cột cho trước cần có trong một log message\n",
    "        \n",
    "        Trả về:\n",
    "        - `headers`: Danh sách tên các cột sử dụng trong DataFrame.\n",
    "        - `regex`: Biểu thức chính quy được sử dụng để phân tích các log message.        \n",
    "        \"\"\"\n",
    "        headers = []\n",
    "        splitters = re.split(r\"(<[^<>]+>)\", logformat)\n",
    "        regex = \"\"\n",
    "        for k in range(len(splitters)):\n",
    "            if k % 2 == 0:\n",
    "                splitter = re.sub(\" +\", \"\\\\\\s+\", splitters[k])\n",
    "                regex += splitter\n",
    "            else:\n",
    "                header = splitters[k].strip(\"<\").strip(\">\")\n",
    "                regex += \"(?P<%s>.*?)\" % header\n",
    "                headers.append(header)\n",
    "        regex = re.compile(\"^\" + regex + \"$\")\n",
    "        return headers, regex\n",
    "\n",
    "    def get_parameter_list(self, row):\n",
    "        \"\"\"\n",
    "        Phương thức được sử dụng để chuyển trích xuất các tham số có trong chuỗi message log. \n",
    "\n",
    "        Tham số:\n",
    "        -------\n",
    "            `row` : Tham số là một đối tượng, yêu cầu có ít nhất 2 thuộc tính `EventTemplate` và `Content`. Trong thuộc tính EventTemplate, <***> chỉ có nhiều nhất 5 kí tự.\n",
    "\n",
    "        Returns:\n",
    "            Danh sách các tham số có trong chuỗi\n",
    "        \"\"\"\n",
    "        template_regex = re.sub(r\"<.{1,5}>\", \"<*>\", row[\"EventTemplate\"])\n",
    "        if \"<*>\" not in template_regex:\n",
    "            return []\n",
    "        template_regex = re.sub(r\"([^A-Za-z0-9])\", r\"\\\\\\1\", template_regex)\n",
    "        template_regex = re.sub(r\"\\\\ +\", r\"\\\\s+\", template_regex)\n",
    "        template_regex = \"^\" + template_regex.replace(\"\\<\\*\\>\", \"(.*?)\") + \"$\"\n",
    "        parameter_list = re.findall(template_regex, row[\"Content\"])\n",
    "        parameter_list = parameter_list[0] if parameter_list else ()\n",
    "        parameter_list = (\n",
    "            list(parameter_list)\n",
    "            if isinstance(parameter_list, tuple)\n",
    "            else [parameter_list]\n",
    "        )\n",
    "        return parameter_list\n",
    "\n",
    "\n",
    "\n",
    "input_dir = path  = './' # The input directory of log file\n",
    "output_dir = 'result/'  # The output directory of parsing results\n",
    "log_file   = 'filelog1copy.log'  # The input log file name\n",
    "log_format = '<Date> <Time>,<Pid> <Level> <Component>: <Content>'  # HDFS log format\n",
    "regex      = [\n",
    "    r'(\\/[\\w\\-.]{2,})+(:[0-9]+)?',                                   #path\n",
    "    r'blk(_[\\-0-9]+){,2}',                                           # blockid\n",
    "    r'0x[a-f0-9]+\\b',                                                #Hexa\n",
    "    r'[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}', #UUID\n",
    "    r'(\\-+[0-9.]+)+\\b',                                              # Block Pool\n",
    "    r'(/|)([0-9]+\\.){3}[0-9]+(:[0-9]+|)(:|)',                        # IP\n",
    "    r'(?<=[^A-Za-z0-9])(\\-?\\+?\\d+)(?=[^A-Za-z0-9])|[0-9]+$',         # Numbers\n",
    "    r'(?<=[^A-Za-z0-9])(\\-?\\+?\\d+)(?=[\\w\\/]+)'                       # Numbers\n",
    "]\n",
    "st         = 0.5 \n",
    "depth      = 4\n",
    "parser = LogParser(log_format, indir=input_dir, outdir=output_dir,  depth=depth, st=st, rex=regex)\n",
    "parser.parse(log_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
