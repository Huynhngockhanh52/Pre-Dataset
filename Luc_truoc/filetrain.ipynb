{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khai báo sử dụng các thư viện. Thực hiện chạy ngay trên CPU, do đó, chỉ thực hiện mô phỏng, nếu ổn định sẽ thực hiện chạy trên kaggle\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import regex as re\n",
    "import os\n",
    "import sys\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "from tabulate import tabulate\n",
    "import copy\n",
    "\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### =========================================================================\n",
    "##### Copyright (C) 2016-2023 LOGPAI (https://github.com/logpai).\n",
    "##### \n",
    "##### Tiếp tục thực hiện chỉnh sửa lại DRAIN sao cho phù hợp với việc phân tích\n",
    "##### Quy trình thực hiện sẽ giữ lại mọt số cấu trúc vào tạo thêm một số cấu trúc, phương thức mới trong DRAIN\n",
    "#### ========================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lớp Logcluster và Node được giữ lại:\n",
    "class Logcluster:\n",
    "    def __init__(self, logTemplate=\"\", logLevel=\"\", logIDL=None):\n",
    "        self.logTemplate = logTemplate\n",
    "        self.logLevel = [logLevel]\n",
    "        if logIDL is None:\n",
    "            logIDL = []\n",
    "        self.logIDL = logIDL\n",
    "        self.logIDLevelL = {logLevel:list(logIDL)}\n",
    "        self.totalOccurrences = {}\n",
    "        \n",
    "    def addIDLevel(self, logLevel, id):\n",
    "        if logLevel not in self.logLevel:\n",
    "            self.logLevel.append(logLevel)\n",
    "            self.logIDLevelL[logLevel] = [id]\n",
    "        else:\n",
    "            self.logIDLevelL[logLevel].append(id)\n",
    "        # if logLevel in self.logLevel:\n",
    "        #     self.logIDLevelL[logLevel].append(id)\n",
    "        # else:\n",
    "        #     self.logIDLevelL[logLevel] = [id]\n",
    "        # self.logIDLevelL.setdefault(logLevel, []).append(id)\n",
    "        \n",
    "    # Phương thức để đặt lại giá trị logLevel, logIDL và logIDLevelL\n",
    "    def resetValues(self):\n",
    "        self.logLevel = []\n",
    "        self.logIDL = []\n",
    "        self.logIDLevelL = {}\n",
    "            \n",
    "\n",
    "class Node:\n",
    "    def __init__(self, childD=None, depth=0, digitOrtoken=None):\n",
    "        if childD is None:\n",
    "            childD = dict()\n",
    "        self.childD = childD\n",
    "        self.depth = depth\n",
    "        self.digitOrtoken = digitOrtoken\n",
    "\n",
    "class LogParser:\n",
    "    def __init__(\n",
    "        self,\n",
    "        log_format,\n",
    "        indir=\"./\",\n",
    "        outdir=\"./result/\",\n",
    "        depth=4,\n",
    "        st=0.4,\n",
    "        maxChild=100,\n",
    "        rex=[],\n",
    "        keep_para=True,\n",
    "        rootNode=Node(),\n",
    "        logClusterList=[]\n",
    "    ):\n",
    "        self.path = indir\n",
    "        self.depth = depth - 2\n",
    "        self.st = st\n",
    "        self.maxChild = maxChild\n",
    "        self.logName = None\n",
    "        self.savePath = outdir\n",
    "        self.df_log = None\n",
    "        self.log_format = log_format\n",
    "        self.rex = rex\n",
    "        self.keep_para = keep_para\n",
    "        self.rootNode = rootNode\n",
    "        self.logClusterList = logClusterList\n",
    "\n",
    "    def hasNumbers(self, s):\n",
    "        return any(char.isdigit() for char in s)\n",
    "\n",
    "    def treeSearch(self, rn, seq):\n",
    "        retLogClust = None\n",
    "\n",
    "        seqLen = len(seq)\n",
    "        if seqLen not in rn.childD:\n",
    "            return retLogClust\n",
    "\n",
    "        parentn = rn.childD[seqLen]\n",
    "\n",
    "        currentDepth = 1\n",
    "        for token in seq:\n",
    "            if currentDepth >= self.depth or currentDepth > seqLen:\n",
    "                break\n",
    "\n",
    "            if token in parentn.childD:\n",
    "                parentn = parentn.childD[token]\n",
    "            elif \"<*>\" in parentn.childD:\n",
    "                parentn = parentn.childD[\"<*>\"]\n",
    "            else:\n",
    "                return retLogClust\n",
    "            currentDepth += 1\n",
    "\n",
    "        logClustL = parentn.childD\n",
    "\n",
    "        retLogClust = self.fastMatch(logClustL, seq)\n",
    "\n",
    "        return retLogClust\n",
    "\n",
    "    def addSeqToPrefixTree(self, rn, logClust):\n",
    "        seqLen = len(logClust.logTemplate)\n",
    "        if seqLen not in rn.childD:\n",
    "            firtLayerNode = Node(depth=1, digitOrtoken=seqLen)\n",
    "            rn.childD[seqLen] = firtLayerNode\n",
    "        else:\n",
    "            firtLayerNode = rn.childD[seqLen]\n",
    "\n",
    "        parentn = firtLayerNode\n",
    "\n",
    "        currentDepth = 1\n",
    "        for token in logClust.logTemplate:\n",
    "            # Add current log cluster to the leaf node\n",
    "            if currentDepth >= self.depth or currentDepth > seqLen:\n",
    "                if len(parentn.childD) == 0:\n",
    "                    parentn.childD = [logClust]\n",
    "                else:\n",
    "                    parentn.childD.append(logClust)\n",
    "                break\n",
    "\n",
    "            # If token not matched in this layer of existing tree.\n",
    "            if token not in parentn.childD:\n",
    "                if not self.hasNumbers(token):\n",
    "                    if \"<*>\" in parentn.childD:\n",
    "                        if len(parentn.childD) < self.maxChild:\n",
    "                            newNode = Node(depth=currentDepth + 1, digitOrtoken=token)\n",
    "                            parentn.childD[token] = newNode\n",
    "                            parentn = newNode\n",
    "                        else:\n",
    "                            parentn = parentn.childD[\"<*>\"]\n",
    "                    else:\n",
    "                        if len(parentn.childD) + 1 < self.maxChild:\n",
    "                            newNode = Node(depth=currentDepth + 1, digitOrtoken=token)\n",
    "                            parentn.childD[token] = newNode\n",
    "                            parentn = newNode\n",
    "                        elif len(parentn.childD) + 1 == self.maxChild:\n",
    "                            newNode = Node(depth=currentDepth + 1, digitOrtoken=\"<*>\")\n",
    "                            parentn.childD[\"<*>\"] = newNode\n",
    "                            parentn = newNode\n",
    "                        else:\n",
    "                            parentn = parentn.childD[\"<*>\"]\n",
    "\n",
    "                else:\n",
    "                    if \"<*>\" not in parentn.childD:\n",
    "                        newNode = Node(depth=currentDepth + 1, digitOrtoken=\"<*>\")\n",
    "                        parentn.childD[\"<*>\"] = newNode\n",
    "                        parentn = newNode\n",
    "                    else:\n",
    "                        parentn = parentn.childD[\"<*>\"]\n",
    "\n",
    "            # If the token is matched\n",
    "            else:\n",
    "                parentn = parentn.childD[token]\n",
    "\n",
    "            currentDepth += 1\n",
    "   \n",
    "    # seq1 is template\n",
    "    def seqDist(self, seq1, seq2):\n",
    "        assert len(seq1) == len(seq2)\n",
    "        simTokens = 0\n",
    "        numOfPar = 0\n",
    "\n",
    "        for token1, token2 in zip(seq1, seq2):\n",
    "            if token1 == \"<*>\":\n",
    "                numOfPar += 1\n",
    "                continue\n",
    "            if token1 == token2:\n",
    "                simTokens += 1\n",
    "\n",
    "        retVal = float(simTokens) / len(seq1)\n",
    "\n",
    "        return retVal, numOfPar\n",
    "\n",
    "    def fastMatch(self, logClustL, seq):\n",
    "        retLogClust = None\n",
    "\n",
    "        maxSim = -1\n",
    "        maxNumOfPara = -1\n",
    "        maxClust = None\n",
    "\n",
    "        for logClust in logClustL:\n",
    "            curSim, curNumOfPara = self.seqDist(logClust.logTemplate, seq)\n",
    "            if curSim > maxSim or (curSim == maxSim and curNumOfPara > maxNumOfPara):\n",
    "                maxSim = curSim\n",
    "                maxNumOfPara = curNumOfPara\n",
    "                maxClust = logClust\n",
    "\n",
    "        if maxSim >= self.st:\n",
    "            retLogClust = maxClust\n",
    "\n",
    "        return retLogClust\n",
    "\n",
    "    def getTemplate(self, seq1, seq2):\n",
    "        assert len(seq1) == len(seq2)\n",
    "        retVal = []\n",
    "\n",
    "        i = 0\n",
    "        for word in seq1:\n",
    "            if word == seq2[i]:\n",
    "                retVal.append(word)\n",
    "            else:\n",
    "                retVal.append(\"<*>\")\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        return retVal\n",
    "\n",
    "    def outputResult(self, logClustL):\n",
    "        log_templates = [0] * self.df_log.shape[0]\n",
    "        log_templateids = [0] * self.df_log.shape[0]\n",
    "        df_events = []\n",
    "        for logClust in logClustL:\n",
    "            template_str = \" \".join(logClust.logTemplate)\n",
    "            occurrence = len(logClust.logIDL)\n",
    "            template_id = hashlib.md5(template_str.encode(\"utf-8\")).hexdigest()[0:8]\n",
    "            for logID in logClust.logIDL:\n",
    "                logID -= 1\n",
    "                log_templates[logID] = template_str\n",
    "                log_templateids[logID] = template_id\n",
    "            df_events.append([template_id, template_str, occurrence])\n",
    "\n",
    "        df_event = pd.DataFrame(\n",
    "            df_events, columns=[\"EventId\", \"EventTemplate\", \"Occurrences\"]\n",
    "        )\n",
    "        self.df_log[\"EventId\"] = log_templateids\n",
    "        self.df_log[\"EventTemplate\"] = log_templates\n",
    "        if self.keep_para:\n",
    "            self.df_log[\"ParameterList\"] = self.df_log.apply(\n",
    "                self.get_parameter_list, axis=1\n",
    "            )\n",
    "        self.df_log.to_csv(\n",
    "            os.path.join(self.savePath, self.logName + \"_structured.csv\"), index=False\n",
    "        )\n",
    "\n",
    "        occ_dict = dict(self.df_log[\"EventTemplate\"].value_counts())\n",
    "        df_event = pd.DataFrame()\n",
    "        df_event[\"EventTemplate\"] = self.df_log[\"EventTemplate\"].unique()\n",
    "        df_event[\"EventId\"] = df_event[\"EventTemplate\"].map(\n",
    "            lambda x: hashlib.md5(x.encode(\"utf-8\")).hexdigest()[0:8]\n",
    "        )\n",
    "        df_event[\"Occurrences\"] = df_event[\"EventTemplate\"].map(occ_dict)\n",
    "        df_event.to_csv(\n",
    "            os.path.join(self.savePath, self.logName + \"_templates.csv\"),\n",
    "            index=False,\n",
    "            columns=[\"EventId\", \"EventTemplate\", \"Occurrences\"],\n",
    "        )\n",
    "\n",
    "    def printTree(self, node, dep):\n",
    "        pStr = \"\"\n",
    "        for i in range(dep):\n",
    "            pStr += \"\\t\"\n",
    "\n",
    "        if node.depth == 0:\n",
    "            pStr += \"Root\"\n",
    "        elif node.depth == 1:\n",
    "            pStr += \"<\" + str(node.digitOrtoken) + \">\"\n",
    "        else:\n",
    "            pStr += node.digitOrtoken\n",
    "\n",
    "        print(pStr)\n",
    "\n",
    "        if node.depth == self.depth:\n",
    "            return 1\n",
    "        for child in node.childD:\n",
    "            self.printTree(node.childD[child], dep + 1)\n",
    "\n",
    "    def parse(self, logName):\n",
    "        print(\"Parsing file: \" + os.path.join(self.path, logName))\n",
    "        start_time = datetime.now()\n",
    "        self.logName = logName\n",
    "        rootNode = Node()\n",
    "        logCluL = []\n",
    "\n",
    "        self.load_data()\n",
    "\n",
    "        count = 0\n",
    "        for idx, line in self.df_log.iterrows():\n",
    "            logID = line[\"LineId\"]\n",
    "            logmessageL = self.preprocess(line[\"Content\"]).strip().split()\n",
    "            matchCluster = self.treeSearch(rootNode, logmessageL)\n",
    "\n",
    "            # Match no existing log cluster\n",
    "            if matchCluster is None:\n",
    "                \n",
    "                \n",
    "                newCluster = Logcluster(logTemplate=logmessageL, logIDL=[logID])\n",
    "                logCluL.append(newCluster)\n",
    "                self.addSeqToPrefixTree(rootNode, newCluster)\n",
    "\n",
    "            # Add the new log message to the existing cluster\n",
    "            else:\n",
    "                newTemplate = self.getTemplate(logmessageL, matchCluster.logTemplate)\n",
    "                matchCluster.logIDL.append(logID)\n",
    "                if \" \".join(newTemplate) != \" \".join(matchCluster.logTemplate):\n",
    "                    matchCluster.logTemplate = newTemplate\n",
    "\n",
    "            count += 1\n",
    "            if count % 1000 == 0 or count == len(self.df_log):\n",
    "                print(\n",
    "                    \"Processed {0:.1f}% of log lines.\".format(\n",
    "                        count * 100.0 / len(self.df_log)\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        if not os.path.exists(self.savePath):\n",
    "            os.makedirs(self.savePath)\n",
    "\n",
    "        self.outputResult(logCluL)\n",
    "\n",
    "        print(\"Parsing done. [Time taken: {!s}]\".format(datetime.now() - start_time))\n",
    "\n",
    "    def load_data(self):\n",
    "        headers, regex = self.generate_logformat_regex(self.log_format)\n",
    "        self.df_log = self.log_to_dataframe(\n",
    "            os.path.join(self.path, self.logName), regex, headers, self.log_format\n",
    "        )\n",
    "\n",
    "    def preprocess(self, line):\n",
    "        for currentRex in self.rex:\n",
    "            line = re.sub(currentRex, \"<*>\", line)\n",
    "        return line\n",
    "\n",
    "    def log_to_dataframe(self, log_file, regex, headers, logformat):\n",
    "        \"\"\"Function to transform log file to dataframe\"\"\"\n",
    "        log_messages = []\n",
    "        linecount = 0\n",
    "        a = []\n",
    "        with open(log_file, \"r\") as fin:\n",
    "            for line in fin.readlines():\n",
    "                try:\n",
    "                    match = regex.search(line.strip())\n",
    "                    message = [match.group(header) for header in headers]\n",
    "                    log_messages.append(message)\n",
    "                    linecount += 1\n",
    "                except Exception as e:\n",
    "                    a.append(linecount)\n",
    "        logdf = pd.DataFrame(log_messages, columns=headers)\n",
    "        logdf.insert(0, \"LineId\", None)\n",
    "        logdf[\"LineId\"] = [i + 1 for i in range(linecount)]\n",
    "        print(\"Total lines: \", len(logdf))\n",
    "        return logdf\n",
    "\n",
    "    def generate_logformat_regex(self, logformat):\n",
    "        \"\"\"Function to generate regular expression to split log messages\"\"\"\n",
    "        headers = []\n",
    "        splitters = re.split(r\"(<[^<>]+>)\", logformat)\n",
    "        regex = \"\"\n",
    "        for k in range(len(splitters)):\n",
    "            if k % 2 == 0:\n",
    "                splitter = re.sub(\" +\", \"\\\\\\s+\", splitters[k])\n",
    "                regex += splitter\n",
    "            else:\n",
    "                header = splitters[k].strip(\"<\").strip(\">\")\n",
    "                regex += \"(?P<%s>.*?)\" % header\n",
    "                headers.append(header)\n",
    "        regex = re.compile(\"^\" + regex + \"$\")\n",
    "        return headers, regex\n",
    "\n",
    "    def get_parameter_list(self, row):\n",
    "        template_regex = re.sub(r\"<.{1,5}>\", \"<*>\", row[\"EventTemplate\"])\n",
    "        if \"<*>\" not in template_regex:\n",
    "            return []\n",
    "        template_regex = re.sub(r\"([^A-Za-z0-9])\", r\"\\\\\\1\", template_regex)\n",
    "        template_regex = re.sub(r\"\\\\ +\", r\"\\\\s+\", template_regex)\n",
    "        template_regex = \"^\" + template_regex.replace(\"\\<\\*\\>\", \"(.*?)\") + \"$\"\n",
    "        parameter_list = re.findall(template_regex, row[\"Content\"])\n",
    "        parameter_list = parameter_list[0] if parameter_list else ()\n",
    "        parameter_list = (\n",
    "            list(parameter_list)\n",
    "            if isinstance(parameter_list, tuple)\n",
    "            else [parameter_list]\n",
    "        )\n",
    "        return parameter_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Một số phương thức hỗ trợ in dữ liệu trong class:\n",
    "\n",
    "# Phương thức in các thuộc tính có trong đối tượng chỉ định\n",
    "def printObj(obj):\n",
    "    print(\"\\n###################################\")\n",
    "    print(\"TYPE: \" + str(type(obj)))\n",
    "    for key, value in vars(obj).items():\n",
    "        if key == \"df_log\":\n",
    "            print(f\"{key}:{len(value)}\")\n",
    "        else:\n",
    "            print(f\"{key}: {value}\")\n",
    "    print(\"###################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing file: ./filelog1.log\n",
      "Total lines:  30000\n",
      "Processed 33.3% of log lines.\n",
      "Processed 66.7% of log lines.\n",
      "Processed 100.0% of log lines.\n",
      "Length DF_EVENTS = 72\n",
      "Parsing done. [Time taken: 0:00:06.688826]\n",
      "72\n",
      "Parsing file: ./filelog2.log\n",
      "Total lines:  33825\n",
      "Processed 29.6% of log lines.\n",
      "Processed 59.1% of log lines.\n",
      "Processed 88.7% of log lines.\n",
      "Processed 100.0% of log lines.\n",
      "Length DF_EVENTS = 83\n",
      "Parsing done. [Time taken: 0:00:07.932148]\n",
      "86\n",
      "Parsing file: ./filelog3.log\n",
      "Total lines:  21399\n",
      "Processed 46.7% of log lines.\n",
      "Processed 93.5% of log lines.\n",
      "Processed 100.0% of log lines.\n",
      "Length DF_EVENTS = 17\n",
      "Parsing done. [Time taken: 0:00:05.263692]\n",
      "89\n",
      "Parsing file: ./filelog4.log\n",
      "Total lines:  41847\n",
      "Processed 23.9% of log lines.\n",
      "Processed 47.8% of log lines.\n",
      "Processed 71.7% of log lines.\n",
      "Processed 95.6% of log lines.\n",
      "Processed 100.0% of log lines.\n",
      "Length DF_EVENTS = 12\n",
      "Parsing done. [Time taken: 0:00:09.004297]\n",
      "89\n",
      "Parsing file: ./filelog5.log\n",
      "Total lines:  175434\n",
      "Processed 5.7% of log lines.\n",
      "Processed 11.4% of log lines.\n",
      "Processed 17.1% of log lines.\n",
      "Processed 22.8% of log lines.\n",
      "Processed 28.5% of log lines.\n",
      "Processed 34.2% of log lines.\n",
      "Processed 39.9% of log lines.\n",
      "Processed 45.6% of log lines.\n",
      "Processed 51.3% of log lines.\n",
      "Processed 57.0% of log lines.\n",
      "Processed 62.7% of log lines.\n",
      "Processed 68.4% of log lines.\n",
      "Processed 74.1% of log lines.\n",
      "Processed 79.8% of log lines.\n",
      "Processed 85.5% of log lines.\n",
      "Processed 91.2% of log lines.\n",
      "Processed 96.9% of log lines.\n",
      "Processed 100.0% of log lines.\n",
      "Length DF_EVENTS = 91\n",
      "Parsing done. [Time taken: 0:00:56.477089]\n",
      "98\n"
     ]
    }
   ],
   "source": [
    "class KeThuaLogParser(LogParser):\n",
    "    def outputResult(self, logClustL):\n",
    "        log_templates = [0] * self.df_log.shape[0]      # Cột Template\n",
    "        log_templateids = [0] * self.df_log.shape[0]    # Cột IDTemplate\n",
    "        df_events = []                                  # DataFrame Events\n",
    "        for logClust in logClustL:\n",
    "            template_str = \" \".join(logClust.logTemplate)\n",
    "            occurrence = len(logClust.logIDL)\n",
    "            template_id = hashlib.md5(template_str.encode(\"utf-8\")).hexdigest()[0:8]\n",
    "            for logID in logClust.logIDL:\n",
    "                logID -= 1\n",
    "                log_templates[logID] = template_str\n",
    "                log_templateids[logID] = template_id\n",
    "            \n",
    "            # Viết thêm dữ liệu:\n",
    "            if len(logClust.logIDLevelL) != 0:\n",
    "                for key, value in logClust.logIDLevelL.items():\n",
    "                    if key in logClust.totalOccurrences:\n",
    "                        logClust.totalOccurrences[key] = logClust.totalOccurrences[key] + len(value)\n",
    "                    else:\n",
    "                        logClust.totalOccurrences[key] = len(value)\n",
    "            \n",
    "            df_events.append([template_id, template_str, occurrence, list(logClust.logLevel), copy.deepcopy(logClust.logIDLevelL)])\n",
    "\n",
    "        df_eventAll = pd.DataFrame(\n",
    "            df_events, columns=[\"EventId\", \"EventTemplate\", \"Occurrences\", \"Level\", \"Details\"]\n",
    "        )\n",
    "        self.df_log[\"EventId\"] = log_templateids\n",
    "        self.df_log[\"EventTemplate\"] = log_templates\n",
    "        if self.keep_para:\n",
    "            self.df_log[\"ParameterList\"] = self.df_log.apply(\n",
    "                self.get_parameter_list, axis=1\n",
    "            )\n",
    "        self.df_log.to_csv(\n",
    "            os.path.join(self.savePath, self.logName + \"_structured.csv\"), index=False\n",
    "        )\n",
    "\n",
    "        occ_dict = dict(self.df_log[\"EventTemplate\"].value_counts())\n",
    "        # Tạo lại một df_event mới chưa có cột nào\n",
    "        df_event = pd.DataFrame()\n",
    "        df_event[\"EventTemplate\"] = self.df_log[\"EventTemplate\"].unique()\n",
    "        df_event[\"EventId\"] = df_event[\"EventTemplate\"].map(\n",
    "            lambda x: hashlib.md5(x.encode(\"utf-8\")).hexdigest()[0:8]\n",
    "        )\n",
    "        df_event[\"Occurrences\"] = df_event[\"EventTemplate\"].map(occ_dict)\n",
    "        # Thêm cột \"Details\" và \"Level\" vào df_event từ df_eventAll dựa trên cột \"EventTemplate\"\n",
    "        df_event = df_event.merge(df_eventAll[[\"EventTemplate\", \"Level\", \"Details\"]], on=\"EventTemplate\", how=\"left\")\n",
    "        # print(df_event[\"Details\"].head(10))\n",
    "        print(f\"Length DF_EVENTS = {df_event.shape[0]}\")\n",
    "        df_event.to_csv(\n",
    "            os.path.join(self.savePath, self.logName + \"_templates.csv\"),\n",
    "            index=False,\n",
    "            columns=[\"EventId\", \"EventTemplate\", \"Occurrences\",\"Level\", \"Details\"],\n",
    "        )\n",
    "        \n",
    "        self.logExtractNgram(self.df_log, \"WARN\", 10)\n",
    "        \n",
    "    def parse(self, logName):\n",
    "        print(\"Parsing file: \" + os.path.join(self.path, logName))\n",
    "        start_time = datetime.now()\n",
    "        self.logName = logName\n",
    "        self.df_log = None\n",
    "        rootNode = self.rootNode\n",
    "        logCluL = self.logClusterList\n",
    "        templateLogCluL = []\n",
    "        for value in logCluL:\n",
    "            templateLogCluL.append(\" \".join(value.logTemplate))\n",
    "        \n",
    "        # ########Load dữ liệu:#############\n",
    "        self.load_data()\n",
    "\n",
    "        count = 0\n",
    "        for idx, line in self.df_log.iterrows():\n",
    "            logID = line[\"LineId\"]\n",
    "            logmessageL = self.preprocess(line[\"Content\"]).strip().split()\n",
    "            logLevelL = str(line['Level'])\n",
    "            matchCluster = self.treeSearch(rootNode, logmessageL)\n",
    "\n",
    "            # Match no existing log cluster\n",
    "            if matchCluster is None:\n",
    "                logMesStr = \" \".join(logmessageL)\n",
    "                numIdx = -1\n",
    "                for idx, element in enumerate(templateLogCluL):\n",
    "                    if logMesStr == element:\n",
    "                        numIdx = idx\n",
    "                        break\n",
    "                if numIdx >= 0:\n",
    "                    matchCluster = logCluL[numIdx]\n",
    "                    newTemplate = self.getTemplate(logmessageL, matchCluster.logTemplate)\n",
    "                    matchCluster.addIDLevel(logLevelL, logID)\n",
    "                    matchCluster.logIDL.append(logID)\n",
    "                    if \" \".join(newTemplate) != \" \".join(matchCluster.logTemplate):\n",
    "                        matchCluster.logTemplate = newTemplate\n",
    "                else:\n",
    "                    newCluster = Logcluster(logTemplate=logmessageL, logLevel=logLevelL, logIDL=[logID])\n",
    "                    logCluL.append(newCluster)\n",
    "                    templateLogCluL.append(\" \".join(logmessageL))\n",
    "                    self.addSeqToPrefixTree(rootNode, newCluster)\n",
    "\n",
    "            # Add the new log message to the existing cluster\n",
    "            else:\n",
    "                newTemplate = self.getTemplate(logmessageL, matchCluster.logTemplate)\n",
    "                matchCluster.addIDLevel(logLevelL, logID)\n",
    "                matchCluster.logIDL.append(logID)\n",
    "                if \" \".join(newTemplate) != \" \".join(matchCluster.logTemplate):\n",
    "                    matchCluster.logTemplate = newTemplate\n",
    "\n",
    "            count += 1\n",
    "            if count % 10000 == 0 or count == len(self.df_log):\n",
    "                print(\n",
    "                    \"Processed {0:.1f}% of log lines.\".format(\n",
    "                        count * 100.0 / len(self.df_log)\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        if not os.path.exists(self.savePath):\n",
    "            os.makedirs(self.savePath)\n",
    "\n",
    "        self.outputResult(logCluL)\n",
    "\n",
    "        for item in logCluL:\n",
    "            item.resetValues()\n",
    "        self.allLogClusterList = logCluL\n",
    "        \n",
    "        print(\"Parsing done. [Time taken: {!s}]\".format(datetime.now() - start_time))\n",
    "        \n",
    "    def logExtractNgram(self, df_root, extract=\"WARN\", n_gram=10):\n",
    "        df_extract = df_root[df_root['Level'] == extract].copy()\n",
    "        if df_extract.shape[0] == 0:\n",
    "            return\n",
    "        df_extract['ListID'] = [[]] * df_extract.shape[0]\n",
    "        df_extract['InfoTemplate'] = [[]] * df_extract.shape[0]\n",
    "        df_extract['InfoPara'] = [[]] * df_extract.shape[0]\n",
    "        for idx, line in df_extract.iterrows():\n",
    "            start_idx = idx - 30 if (idx - 30 >= 0) else 0\n",
    "            df_select = None\n",
    "            while(1):\n",
    "                data_root = df_root.iloc[start_idx:idx]\n",
    "                df_select = data_root[data_root['Level'] == \"INFO\"]\n",
    "                if df_select.shape[0] >= 10 or start_idx == 0:\n",
    "                    df_select = df_select.iloc[-10:]\n",
    "                    break\n",
    "                else:\n",
    "                    start_idx = start_idx - 20 if (start_idx - 20 >= 0) else 0\n",
    "            df_extract.at[idx, 'ListID'] = df_select[\"LineId\"].tolist()\n",
    "            df_extract.at[idx, 'InfoTemplate'] = df_select[\"EventId\"].tolist()\n",
    "            df_extract.at[idx, 'InfoPara'] = df_select[\"ParameterList\"].tolist()\n",
    "            \n",
    "        df_extract.to_csv(\n",
    "            os.path.join(self.savePath, self.logName + \"_\" + extract + \"_out.csv\"), index=False, sep='|'\n",
    "        )\n",
    "    \n",
    "    # def writingExcelSheet(self, df_root, extract=\"WARN\"):\n",
    "    #     out_file = extract + \"_out_template.xlsx\"\n",
    "    #     unique_df = df_root['EventId'].unique()\n",
    "    #     # Đọc file Excel hiện có và lấy tên các sheet đã tồn tại\n",
    "    #     try:\n",
    "    #         wb = load_workbook(out_file)\n",
    "    #     except FileNotFoundError:\n",
    "    #         wb = Workbook()\n",
    "\n",
    "    #     existing_sheets = wb.sheetnames\n",
    "\n",
    "    #     for template_id in unique_df:\n",
    "    #         if str(template_id) in existing_sheets:\n",
    "    #             # Nếu sheet đã tồn tại, mở sheet đó và thêm dữ liệu\n",
    "    #             ws = wb[str(template_id)]\n",
    "    #         else:\n",
    "    #             # Nếu sheet chưa tồn tại, tạo mới sheet đó và thêm dữ liệu\n",
    "    #             ws = wb.create_sheet(title=str(template_id))\n",
    "\n",
    "    #         data_add = df_root[df_root['EventId'] == template_id]\n",
    "\n",
    "    #         for r_idx, row in enumerate(dataframe_to_rows(data_add, index=False, header=True), 1):\n",
    "    #             ws.append(row)\n",
    "    #     wb.save(out_file)\n",
    "        \n",
    "            \n",
    "\n",
    "input_dir = path  = './' # The input directory of log file\n",
    "output_dir = 'result/'  # The output directory of parsing results\n",
    "log_file   = 'filelog1.log'  # The input log file name\n",
    "log_format = '<Date> <Time>,<Pid> <Level> <Component>: <Content>'  # HDFS log format\n",
    "regex      = [\n",
    "    r'(\\/[\\w\\-.]{2,})+(:[0-9]+)?',                                   #path\n",
    "    r'blk(_[\\-0-9]+){,2}',                                          # blockid\n",
    "    r'0x[a-f0-9]+\\b',                                                #Hexa\n",
    "    r'[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}', #UUID\n",
    "    r'(\\-+[0-9.]+)+\\b',                                                 # Block Pool\n",
    "    r'(/|)([0-9]+\\.){3}[0-9]+(:[0-9]+|)(:|)',                        # IP\n",
    "    r'(?<=[^A-Za-z0-9])(\\-?\\+?\\d+)(?=[^A-Za-z0-9])|[0-9]+$',         # Numbers\n",
    "    r'(?<=[^A-Za-z0-9])(\\-?\\+?\\d+)(?=[\\w\\/]+)'                       # Numbers\n",
    "]\n",
    "st         = 0.5 \n",
    "depth      = 4\n",
    "allRootNode = Node()\n",
    "allLogClusterList = []\n",
    "parser = KeThuaLogParser(log_format, indir=input_dir, outdir=output_dir,  depth=depth, st=st, rex=regex, rootNode=allRootNode, logClusterList=allLogClusterList)\n",
    "\n",
    "# allLogClusterList = parser.logClusterList\n",
    "# parser.parse(log_file)\n",
    "for i in range(1,6):\n",
    "    log_file = \"filelog\" + str(i) + \".log\"\n",
    "    parser.parse(log_file)\n",
    "    print(len(allLogClusterList))\n",
    "\n",
    "\n",
    "# print(len(allLogClusterList))\n",
    "# log_file = 'filelog2.log'\n",
    "# parser.parse(log_file)\n",
    "# print(len(allLogClusterList))\n",
    "\n",
    "# for obj in allLogClusterList:\n",
    "#     for key, value in vars(obj).items():\n",
    "#         if key == \"logTemplate\":\n",
    "#             print(f\"{key}: {str(''.join(value))}\")\n",
    "#         # if key == \"totalOccurrences\":\n",
    "#         #     print(f\"\\t{key}:\")\n",
    "#         #     for k,v in value.items():\n",
    "#         #         print(f\"\\t\\t{k}: {v}\")\n",
    "\n",
    "# parser.printTree(allRootNode, 4)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chương trình chỉnh sửa sẽ sử dụng lại các thành phần ở trên, sử dụng lại 2 class: LogCluster và Node:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
